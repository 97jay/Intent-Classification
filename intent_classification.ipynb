{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jayanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jayanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jayanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Required headerfiles\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class that encasulates variables and methods for training our model\n",
    "\n",
    "In this task we are given a JSON file that contains sentences along with its correspoding intent label. We need to train a model that can take a sentence and classify which intent the sentence belongs to. For this task I am using only 20 random intents for training and testing from a list of 150 intent classes. \n",
    "\n",
    "Firstly, I preprocess the data. We are given separate data for training, testing and validation. The first step in preprocessing is to filter out special characters from the sentence and convert each sentence to a list of words after lower casing all the letters in the words (using white space tokenization). I also applied lemmatization to all the words(convert to root form). These things are done to help in analysing the text as a sequence of words to interpret the meaning. I did not remove any stop words to have some noise in the dataset(may help in generalization). We need to find out the maximum length of a sentence from all the given sentences to help in the padding process. The second step involves creating a vocabulary from all these words to encode them. I created a vocabulary index based on the word frequency. It actually maps each word to an integer; lower the integer for a word, more frequent the word has occured. After this step we apply this tokenizer to all the words in a sentence thus creating a list of integers for all the words in a sentence. Since each sentence is of different length, I pad the sentence with 0 to make length of all sentences same and equal to the maximum length that we had found out earlier(For embedding layer that I am going to use, the sentence length should be equal).\n",
    "\n",
    "I apply the same techniques to the labels as well but here I consider the full label as a word. The last step of preprocessing is to encode the labels using one hot encoder, since we have 20 classes and this technique helps in representing the data in a usable form for the model to perform training.\n",
    "\n",
    "The data that we receive after performing tokenization, encoding and padding serves as the input to the model. The model I use over here consists of an embedding layer. This embedding layer helps to embed each word into a continuous vector space. These embeddings act as extracted feature representation. I use a Bidirectional LSTM layer because it can understand the context better. Vanilla RNN has the problem of vanishing/exploding gradients problem and I felt the Bidirectional LSTM would be an ideal network for this task (since it can encode sequence in both forward as well as backward direction) thus understanding the context better for the intent classification task. \n",
    "\n",
    "I used accuracy as the metric to measure the performance of the model. I also build a confusion matrix for this multiclass problem since we can get to see if any particular group of classes are not performing well. \n",
    "\n",
    "Since I was not fully familiar with text based modelling, it was a nice challenge for me to understand various things that are performed on text like filtering the text, removing the stop words, performing lemmatization, building vocabulary etc. I have written various helper functions to make the code look small and thus simplified it. I was thinking of using better models like BERT in future implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class intentClassification():\n",
    "    \n",
    "    def __init__(self, train_data,val_data,test_data):\n",
    "        #Creating a dictionary to store the data with intent as key and sentences as values\n",
    "        self.train_dict=self.create_dictionary(train_data)\n",
    "        self.val_dict=self.create_dictionary(val_data)\n",
    "        self.test_dict=self.create_dictionary(test_data)\n",
    "        self.training_testing()\n",
    "\n",
    "    #function to train the model\n",
    "    def training_testing(self):\n",
    "        \n",
    "        #To randomly sample any 20 intent classes for the purpose of training. Storing them for purpose of loading them\n",
    "        # in case of using pretrained model.\n",
    "        \n",
    "        #********************Important - If you want to train your model for different set of 20 Intents please set new_intents\n",
    "        #variable below as 1. If you want to use the pretrained model please set new_intents variable as 0,\n",
    "        #and just load model from the file Intent.txt(model load code will be below)***************************\n",
    "        \n",
    "        new_intents=0\n",
    "        if(new_intents):\n",
    "            self.train_random_20_data = random.sample(list(self.train_dict), 20) \n",
    "            with open('./intent.txt', 'w') as f:\n",
    "                for item in self.train_random_20_data:\n",
    "                    f.write(\"%s\\n\" % item)\n",
    "        else:\n",
    "            with open('./intent.txt') as f:\n",
    "                self.train_random_20_data = f.read().splitlines()\n",
    "        \n",
    "        #Printing the intents(labels) I am using\n",
    "        print(self.train_random_20_data)\n",
    "\n",
    "        #Storing the 20 intent classes and its corresponding sentences\n",
    "        train_rows=self.create_rows(self.train_dict)\n",
    "        val_rows=self.create_rows(self.val_dict)\n",
    "        test_rows=self.create_rows(self.test_dict)\n",
    "        \n",
    "        #Using data frame to store the data from the above lists with columns sentences and its corresponding intent label\n",
    "        df_train = pd.DataFrame(train_rows, columns=[\"Sentence\", \"Intent\"])   \n",
    "        df_val=pd.DataFrame(val_rows, columns=[\"Sentence\", \"Intent\"])\n",
    "        df_test=pd.DataFrame(test_rows, columns=[\"Sentence\", \"Intent\"])\n",
    "        \n",
    "        #List of intents I am using\n",
    "        unique_intents = list(set(df_train['Intent']))\n",
    "        \n",
    "        #To remove the special characters and convert sentences into list of words\n",
    "        s_train=list(df_train['Sentence'])\n",
    "        s_val=list(df_val['Sentence'])\n",
    "        s_test=list(df_test['Sentence'])\n",
    "        clean_words_train = self.tokenize_to_words(s_train)\n",
    "        clean_words_val = self.tokenize_to_words(s_val)\n",
    "        clean_words_test = self.tokenize_to_words(s_test)\n",
    "        #print(len(clean_words_train))\n",
    "        #print(clean_words_train[:2])\n",
    "        \n",
    "        #To find out the maximum length from all the sentences(maximum number of words)\n",
    "        max_length = len(max(clean_words_train, key = len))\n",
    "        \n",
    "        #Using tokenizer to create a vocabulary based on word frequency\n",
    "        word_tokenizer = self.create_tokenizer(clean_words_train)\n",
    "        \n",
    "        #Size of the vocabulary\n",
    "        total_vocabulary_size = len(word_tokenizer.word_index) + 1\n",
    "        \n",
    "        #To transform each text in a sentence to a sequence of integers using tokenizer. After doing this I pad all the \n",
    "        #sentenes at the end of sentences by appending 0 to make length of all sentences same and equal to length of the \n",
    "        #longest sentence. The is done for train, test and validation data\n",
    "        encoded_doc_train = word_tokenizer.texts_to_sequences(clean_words_train)\n",
    "        padded_doc_train = pad_sequences(encoded_doc_train, maxlen=max_length,padding = \"post\")\n",
    "        \n",
    "        encoded_doc_val = word_tokenizer.texts_to_sequences(clean_words_val)\n",
    "        padded_doc_val = pad_sequences(encoded_doc_val, maxlen=max_length,padding = \"post\")\n",
    "        \n",
    "        encoded_doc_test = word_tokenizer.texts_to_sequences(clean_words_test)\n",
    "        padded_doc_test = pad_sequences(encoded_doc_test, maxlen=max_length,padding = \"post\")\n",
    "        \n",
    "        #Use the tokenizer for the labels to convert them to integer for train,test and validation data\n",
    "        output_tokenizer = self.create_tokenizer(unique_intents,'!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~\\t\\n')\n",
    "        #print(output_tokenizer.word_index)\n",
    "        encoded_output_train = output_tokenizer.texts_to_sequences(df_train[\"Intent\"])\n",
    "        encoded_output_train = np.array(encoded_output_train).reshape(len(encoded_output_train), 1)\n",
    "        \n",
    "        encoded_output_val = output_tokenizer.texts_to_sequences(df_val[\"Intent\"])\n",
    "        encoded_output_val = np.array(encoded_output_val).reshape(len(encoded_output_val), 1)\n",
    "        \n",
    "        encoded_output_test = output_tokenizer.texts_to_sequences(df_test[\"Intent\"])\n",
    "        encoded_output_test = np.array(encoded_output_test).reshape(len(encoded_output_test), 1)\n",
    "        \n",
    "        #Since we have 20 classes I make use of one hot encoder to represent the categorical data to a more usable form for \n",
    "        #the model to use them for training purpose\n",
    "        output_one_hot_encoded_train = self.one_hot_encode(encoded_output_train)\n",
    "        output_one_hot_encoded_train.shape\n",
    "        \n",
    "        output_one_hot_encoded_val = self.one_hot_encode(encoded_output_val)\n",
    "        output_one_hot_encoded_val.shape\n",
    "        \n",
    "        output_one_hot_encoded_test = self.one_hot_encode(encoded_output_test)\n",
    "        output_one_hot_encoded_test.shape\n",
    "        \n",
    "        #Preparing data for feeding into the model\n",
    "        train_X,train_Y=padded_doc_train,output_one_hot_encoded_train\n",
    "        val_X,val_Y=padded_doc_val,output_one_hot_encoded_val\n",
    "        test_X,test_Y=padded_doc_test,output_one_hot_encoded_test\n",
    "        \n",
    "        #Creating the model and using categorical crossentropy as our loss function with adam optimizer\n",
    "        model = self.create_model(total_vocabulary_size, max_length)\n",
    "        model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "        model.summary()\n",
    "        \n",
    "        #Training the model\n",
    "        #*****************************Specify if you want to train the model by setting is_train as 1 else 0*******************\n",
    "        is_train=1\n",
    "        if(is_train):\n",
    "            hist = model.fit(train_X, train_Y, epochs = 100, batch_size = 32, validation_data = (val_X, val_Y))\n",
    "        \n",
    "        #Saving the trained model\n",
    "        #**********************If you want to save your trained model, uncomment the next line******************************* \n",
    "        model.save('./model')\n",
    "        \n",
    "        #Use a pretrained model\n",
    "        #**********************If you want to use the pretrained model, uncomment the next line****************************** \n",
    "        #model = keras.models.load_model('./model')\n",
    "\n",
    "        \n",
    "        #print(model.metrics_names)\n",
    "        scores = model.evaluate(test_X, test_Y, verbose=1)\n",
    "        #print(scores)\n",
    "        print(\"\\nTesting Accuracy : \"+str(scores[1]*100))\n",
    "        \n",
    "        #For building confusion matrix \n",
    "        pred = model.predict_proba(test_X)\n",
    "        f=0\n",
    "        Actual=[]\n",
    "        Predicted=[]\n",
    "        for i in range(len(pred)):\n",
    "            part=np.argmax(pred[i])\n",
    "            Actual.append(test_rows[i][1])\n",
    "            Predicted.append(unique_intents[part])\n",
    "            if(test_rows[i][1]!=unique_intents[part]):\n",
    "                f+=1\n",
    "        print(\"\\nConfusion Matrix\")       \n",
    "        print(metrics.confusion_matrix(Actual, Predicted))\n",
    "        print(\"\\nClassification Report\")\n",
    "        print(metrics.classification_report(Actual, Predicted, digits=3))\n",
    "\n",
    "\n",
    "    #This function denotes the model I have used for classification.\n",
    "    def create_model(self,vocab_size, max_length):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, 256, input_length = max_length, trainable = False))   \n",
    "        model.add(Bidirectional(LSTM(256)))\n",
    "        model.add(Dense(32, activation = \"relu\"))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(20, activation = \"softmax\"))\n",
    "        return model  \n",
    "    \n",
    "    #This function helps in encoding the intents using One hot encoder(20 classes)\n",
    "    def one_hot_encode(self,encode):\n",
    "        temp = OneHotEncoder(sparse = False)\n",
    "        return(temp.fit_transform(encode))\n",
    "   \n",
    "    #This function creates a vocabulary index based on the word frequency and taking into consideration all the filters that\n",
    "    #are specified. Each word is mapped to an integer and lower the integer for a word more frequent the word has occured.\n",
    "    def create_tokenizer(self,words,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "        token = Tokenizer(filters = filters)   \n",
    "        token.fit_on_texts(words)\n",
    "        return token\n",
    "    \n",
    "    #This function is used to remove any special characters, lemmatize the words, lowercase the letters and convert \n",
    "    #sentences into a list of words\n",
    "    def tokenize_to_words(self,sentences):\n",
    "        words = []\n",
    "        for s in sentences:\n",
    "            removal = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "            w = word_tokenize(removal)\n",
    "            words.append([lemmatizer.lemmatize(i.lower()) for i in w])\n",
    "        return words\n",
    "\n",
    "    #To create dictionary that stores the intent and all the sentences of that intent for train,test and validation data\n",
    "    def create_dictionary(self,data):\n",
    "        temp_dict={}\n",
    "        for i in range(len(data)):\n",
    "            if (data[i][1] not in temp_dict):\n",
    "                temp_dict[data[i][1]] = []\n",
    "            temp_dict[data[i][1]].append(data[i][0])\n",
    "        return temp_dict\n",
    "    \n",
    "    #This helper function is used for aiding the creation of dataframe for train, test and validation dataset from dictionary by\n",
    "    #randomly sampling 20 intent classes\n",
    "    def create_rows(self,temp_dict):\n",
    "        rows=[]\n",
    "        for i in self.train_random_20_data:\n",
    "            for j in temp_dict[i]:\n",
    "                a=[]\n",
    "                a.append(j)\n",
    "                a.append(i)\n",
    "                rows.append(a)\n",
    "        return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Function\n",
    "We have an intent classification dataset in the form of a JSON file. In the main function I read the training, testing and validation data from the dataset and call the training class to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['todo_list', 'redeem_rewards', 'order_checks', 'bill_balance', 'where_are_you_from', 'distance', 'pin_change', 'book_flight', 'improve_credit_score', 'next_holiday', 'change_accent', 'date', 'who_do_you_work_for', 'report_lost_card', 'flip_coin', 'application_status', 'shopping_list_update', 'restaurant_reviews', 'taxes', 'whisper_mode']\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 26, 256)           343040    \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 512)               1050624   \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 20)                660       \n",
      "=================================================================\n",
      "Total params: 1,410,740\n",
      "Trainable params: 1,067,700\n",
      "Non-trainable params: 343,040\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 2.9635 - accuracy: 0.0780 - val_loss: 2.8907 - val_accuracy: 0.1750\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.6564 - accuracy: 0.1735 - val_loss: 2.4783 - val_accuracy: 0.3850\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.3045 - accuracy: 0.2735 - val_loss: 1.9218 - val_accuracy: 0.5250\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 1.8851 - accuracy: 0.4125 - val_loss: 1.5326 - val_accuracy: 0.6350\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 1.5293 - accuracy: 0.5165 - val_loss: 1.1434 - val_accuracy: 0.7150\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 1.2236 - accuracy: 0.6145 - val_loss: 0.8270 - val_accuracy: 0.7800\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 1.0431 - accuracy: 0.6675 - val_loss: 0.7990 - val_accuracy: 0.8250\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.7780 - accuracy: 0.7570 - val_loss: 0.6619 - val_accuracy: 0.8450\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.7304 - accuracy: 0.7780 - val_loss: 0.5834 - val_accuracy: 0.8725\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.5956 - accuracy: 0.8160 - val_loss: 0.4687 - val_accuracy: 0.8875\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.5095 - accuracy: 0.8480 - val_loss: 0.4956 - val_accuracy: 0.8775\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.5046 - accuracy: 0.8520 - val_loss: 0.6110 - val_accuracy: 0.8350\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.5296 - accuracy: 0.8395 - val_loss: 0.4096 - val_accuracy: 0.9125\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.3941 - accuracy: 0.8780 - val_loss: 0.3850 - val_accuracy: 0.9025\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.3479 - accuracy: 0.8845 - val_loss: 0.3813 - val_accuracy: 0.9150\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.3430 - accuracy: 0.8925 - val_loss: 0.6167 - val_accuracy: 0.8625\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.4108 - accuracy: 0.8765 - val_loss: 0.3862 - val_accuracy: 0.9025\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.3010 - accuracy: 0.9115 - val_loss: 0.3414 - val_accuracy: 0.9375\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.2604 - accuracy: 0.9140 - val_loss: 0.3640 - val_accuracy: 0.9225\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.2435 - accuracy: 0.9170 - val_loss: 0.3565 - val_accuracy: 0.9275\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.2262 - accuracy: 0.9285 - val_loss: 0.4242 - val_accuracy: 0.9325\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1762 - accuracy: 0.9385 - val_loss: 0.4286 - val_accuracy: 0.9275\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.2034 - accuracy: 0.9260 - val_loss: 0.4125 - val_accuracy: 0.9275\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1713 - accuracy: 0.9430 - val_loss: 0.4510 - val_accuracy: 0.9375\n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1949 - accuracy: 0.9260 - val_loss: 0.4504 - val_accuracy: 0.9275\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.2670 - accuracy: 0.9215 - val_loss: 0.4681 - val_accuracy: 0.9125\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.2632 - accuracy: 0.9220 - val_loss: 0.4381 - val_accuracy: 0.9250\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.2402 - accuracy: 0.9280 - val_loss: 0.4794 - val_accuracy: 0.9150\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1967 - accuracy: 0.9300 - val_loss: 0.3550 - val_accuracy: 0.9350\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1841 - accuracy: 0.9360 - val_loss: 0.3789 - val_accuracy: 0.9325\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1675 - accuracy: 0.9410 - val_loss: 0.4456 - val_accuracy: 0.9350\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1473 - accuracy: 0.9470 - val_loss: 0.4328 - val_accuracy: 0.9300\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1598 - accuracy: 0.9475 - val_loss: 0.2974 - val_accuracy: 0.9275\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1159 - accuracy: 0.9580 - val_loss: 0.3622 - val_accuracy: 0.9450\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1120 - accuracy: 0.9615 - val_loss: 0.4398 - val_accuracy: 0.9350\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1075 - accuracy: 0.9565 - val_loss: 0.3734 - val_accuracy: 0.9450\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1209 - accuracy: 0.9540 - val_loss: 0.4281 - val_accuracy: 0.9375\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1223 - accuracy: 0.9580 - val_loss: 0.3478 - val_accuracy: 0.9425\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1004 - accuracy: 0.9620 - val_loss: 0.4296 - val_accuracy: 0.9400\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1021 - accuracy: 0.9585 - val_loss: 0.4548 - val_accuracy: 0.9400\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1415 - accuracy: 0.9510 - val_loss: 0.5240 - val_accuracy: 0.9325\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1654 - accuracy: 0.9455 - val_loss: 0.2947 - val_accuracy: 0.9350\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1178 - accuracy: 0.9560 - val_loss: 0.4779 - val_accuracy: 0.9400\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1017 - accuracy: 0.9590 - val_loss: 0.3575 - val_accuracy: 0.9500\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0885 - accuracy: 0.9655 - val_loss: 0.5296 - val_accuracy: 0.9350\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0962 - accuracy: 0.9675 - val_loss: 0.4590 - val_accuracy: 0.9450\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0804 - accuracy: 0.9705 - val_loss: 0.4221 - val_accuracy: 0.9525\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0800 - accuracy: 0.9725 - val_loss: 0.5018 - val_accuracy: 0.9475\n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0879 - accuracy: 0.9675 - val_loss: 0.4770 - val_accuracy: 0.9425\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1241 - accuracy: 0.9595 - val_loss: 0.4837 - val_accuracy: 0.9250\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.2633 - accuracy: 0.9300 - val_loss: 0.5740 - val_accuracy: 0.9250\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1837 - accuracy: 0.9495 - val_loss: 0.5224 - val_accuracy: 0.9375\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.2427 - accuracy: 0.9450 - val_loss: 0.6158 - val_accuracy: 0.8700\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.5588 - accuracy: 0.8795 - val_loss: 0.3425 - val_accuracy: 0.9250\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.2126 - accuracy: 0.9450 - val_loss: 0.3610 - val_accuracy: 0.9250\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1748 - accuracy: 0.9460 - val_loss: 0.4617 - val_accuracy: 0.9200\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1190 - accuracy: 0.9580 - val_loss: 0.4282 - val_accuracy: 0.9375\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0929 - accuracy: 0.9650 - val_loss: 0.4267 - val_accuracy: 0.9425\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0901 - accuracy: 0.9690 - val_loss: 0.4393 - val_accuracy: 0.9400\n",
      "Epoch 60/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1085 - accuracy: 0.9595 - val_loss: 0.3168 - val_accuracy: 0.9500\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1206 - accuracy: 0.9600 - val_loss: 0.4025 - val_accuracy: 0.9425\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1644 - accuracy: 0.9575 - val_loss: 0.3769 - val_accuracy: 0.9400\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1515 - accuracy: 0.9520 - val_loss: 0.4943 - val_accuracy: 0.9275\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1464 - accuracy: 0.9510 - val_loss: 0.5486 - val_accuracy: 0.9375\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.2288 - accuracy: 0.9445 - val_loss: 0.3648 - val_accuracy: 0.9525\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.0890 - accuracy: 0.9695 - val_loss: 0.5214 - val_accuracy: 0.9350\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0846 - accuracy: 0.9680 - val_loss: 0.4200 - val_accuracy: 0.9500\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0765 - accuracy: 0.9680 - val_loss: 0.3495 - val_accuracy: 0.9550\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0804 - accuracy: 0.9720 - val_loss: 0.4562 - val_accuracy: 0.9475\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0774 - accuracy: 0.9735 - val_loss: 0.4098 - val_accuracy: 0.9450\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0583 - accuracy: 0.9770 - val_loss: 0.5065 - val_accuracy: 0.9500\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0458 - accuracy: 0.9835 - val_loss: 0.5356 - val_accuracy: 0.9500\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.0723 - accuracy: 0.9760 - val_loss: 0.5283 - val_accuracy: 0.9425\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0791 - accuracy: 0.9725 - val_loss: 0.5148 - val_accuracy: 0.9525\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0564 - accuracy: 0.9815 - val_loss: 0.5518 - val_accuracy: 0.9425\n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0577 - accuracy: 0.9805 - val_loss: 0.5900 - val_accuracy: 0.9475\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.0595 - accuracy: 0.9740 - val_loss: 0.6475 - val_accuracy: 0.9450\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0548 - accuracy: 0.9735 - val_loss: 0.5703 - val_accuracy: 0.9475\n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0663 - accuracy: 0.9735 - val_loss: 0.5884 - val_accuracy: 0.9525\n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0527 - accuracy: 0.9810 - val_loss: 0.6266 - val_accuracy: 0.9575\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0654 - accuracy: 0.9750 - val_loss: 0.5696 - val_accuracy: 0.9525\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0673 - accuracy: 0.9735 - val_loss: 0.5596 - val_accuracy: 0.9500\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0531 - accuracy: 0.9800 - val_loss: 0.6294 - val_accuracy: 0.9425\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0569 - accuracy: 0.9780 - val_loss: 0.6872 - val_accuracy: 0.9400\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0568 - accuracy: 0.9805 - val_loss: 0.6909 - val_accuracy: 0.9450\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.0575 - accuracy: 0.9750 - val_loss: 0.6571 - val_accuracy: 0.9400\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0583 - accuracy: 0.9745 - val_loss: 0.7454 - val_accuracy: 0.9425\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.0564 - accuracy: 0.9780 - val_loss: 0.7911 - val_accuracy: 0.9375\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.0494 - accuracy: 0.9815 - val_loss: 0.7114 - val_accuracy: 0.9425\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.0508 - accuracy: 0.9810 - val_loss: 0.7951 - val_accuracy: 0.9400\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0599 - accuracy: 0.9805 - val_loss: 0.5940 - val_accuracy: 0.9400\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0877 - accuracy: 0.9775 - val_loss: 0.7626 - val_accuracy: 0.9350\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0637 - accuracy: 0.9740 - val_loss: 0.6562 - val_accuracy: 0.9450\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1453 - accuracy: 0.9625 - val_loss: 0.3510 - val_accuracy: 0.9250\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.3076 - accuracy: 0.9380 - val_loss: 0.6362 - val_accuracy: 0.9125\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.3040 - accuracy: 0.9370 - val_loss: 0.3227 - val_accuracy: 0.9350\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1453 - accuracy: 0.9635 - val_loss: 0.4172 - val_accuracy: 0.9400\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1113 - accuracy: 0.9640 - val_loss: 0.4046 - val_accuracy: 0.9475\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0642 - accuracy: 0.9770 - val_loss: 0.5254 - val_accuracy: 0.9525\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0784 - accuracy: 0.9750 - val_loss: 0.3869 - val_accuracy: 0.9525\n",
      "600/600 [==============================] - 1s 2ms/step\n",
      "\n",
      "Testing Accuracy : 94.33333277702332\n",
      "\n",
      "Confusion Matrix\n",
      "[[30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 28  0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  0 30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 27  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0]\n",
      " [ 0  0  0  0 28  0  0  1  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  1  0  0 26  0  0  0  0  0  0  0  2  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  0  0  0  0  0  0 26  0  0  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 29  0  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 30  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0 27  2  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  1  0  0  2 26  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  1 28  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0 29  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  1  0  0 28  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0 28  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 30  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0 29  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 30  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  0  0  1  0  0  0  0  1 27]]\n",
      "\n",
      "Classification Report\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "  application_status      0.833     1.000     0.909        30\n",
      "        bill_balance      0.933     0.933     0.933        30\n",
      "         book_flight      0.968     1.000     0.984        30\n",
      "       change_accent      1.000     0.900     0.947        30\n",
      "                date      0.966     0.933     0.949        30\n",
      "            distance      1.000     0.867     0.929        30\n",
      "           flip_coin      0.968     1.000     0.984        30\n",
      "improve_credit_score      0.929     0.867     0.897        30\n",
      "        next_holiday      1.000     0.967     0.983        30\n",
      "        order_checks      0.968     1.000     0.984        30\n",
      "          pin_change      0.871     0.900     0.885        30\n",
      "      redeem_rewards      0.867     0.867     0.867        30\n",
      "    report_lost_card      1.000     0.933     0.966        30\n",
      "  restaurant_reviews      0.906     0.967     0.935        30\n",
      "shopping_list_update      1.000     0.933     0.966        30\n",
      "               taxes      0.933     0.933     0.933        30\n",
      "           todo_list      0.938     1.000     0.968        30\n",
      "  where_are_you_from      1.000     0.967     0.983        30\n",
      "        whisper_mode      0.857     1.000     0.923        30\n",
      " who_do_you_work_for      1.000     0.900     0.947        30\n",
      "\n",
      "            accuracy                          0.943       600\n",
      "           macro avg      0.947     0.943     0.944       600\n",
      "        weighted avg      0.947     0.943     0.944       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with open('./data_full.json') as f:\n",
    "        data = json.load(f)\n",
    "    train_data=data['train']\n",
    "    test_data=data['test']\n",
    "    val_data=data['val']\n",
    "    intentClassification(train_data,val_data,test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
